% No hay más información que la información de Shannon
% Manolo Martínez (Antwerp)
% Valencia Colloquium -- Diciembre 2017

# Introduction

## Perceptual content from below

* Two ways (among several) to get to the representational content of
  perceptions:

1. From above 
    - Reports of perceptual judgements
    - Action guidance

2. From below
    - Applying a metasemantic theory to perceptual vehicles

## The liberality problem

* The core metasemantics: content is constituted by information (plus function)
  [@millikan_language_1984; @shea_representation_forth; @skyrms_signals_2010] 

* A problem in the approach from below: 
    - Focusing on information (plus function) results in liberal accounts,
      which credit the states of very simple systems with representational
      content.

* In such systems, representational content has no explanatory traction
  [@burge_origins_2010; @rescorla_millikan_2013; @schulte_perceptual_2015]

## A common fix

* Representational content only inheres in systems which, over and above
  carrying information about distal states of affairs, are *many to one* and/or
  *one to many* in various ways:
    - Many to one: Perceptual constancies (Burge, Schulte)
    - One to many; Broad-banded response (Sterelny)

## A better fix 

* A better fix to the liberality problem in information-based accounts is...
  more information.

* Typical information-theoretic models of representation are too simplistic.
    - Once we (slightly) increase their expressive power, several key
      properties come into focus.

## The proposal in a nutshell

* A family of paradigmatically representational systems:

1. Information passes through a *bottleneck*
2. There is a *sweet spot* in the trade-off between accuracy and the width of
   the bottleneck
3. The system operates near the sweet spot.

## The scope of the proposal

* These three conditions are neither necessary nor (perhaps?) sufficient) for
  representational status.
    - They merely characterize a central case of representation in simple
      systems.

* When they hold, representational talk has explanatory traction. Many-to-one-to-many
  constraints fall out naturally from the informational setup.
    - They are not really additional constraints on representational systems


# Some preliminary concepts

## The Lewis-Skyrms main model

![](graphviz/sender-receiver-onepayoff.pdf){scale=0.5}

## $P(Message | Sender)$

\begin{table}[!ht]
    \centering
\begin{tabular}{|c||c|c|c|}
& $M_{1}$ & $M_{2}$ & $M_{3}$ \\
\hline
$W_{1}$ & .5 & .25 & .25 \\
$W_{2}$ & .33 & .33 & .33 \\
$W_{3}$ & .6 & .4 & .2 \\
\end{tabular}
\caption{A sender strategy}
\end{table}

## Signaling games (i)

\begin{table}[!ht]
    \centering
\begin{tabular}{|c||c|c|c|}
& $S_{1}$ & $S_{2}$ & $S_{3}$ \\
\hline
$A_{1}$ & 5,0 & 2,4 & 0,5 \\
$A_{2}$ & 6,5 & 0,0 & 1,6 \\
$A_{3}$ & 0,6 & 6,6 & 5,3 \\
\end{tabular}
\caption{Sender and receiver payoff matrices}
\end{table}

## The Lewis-Skyrms main model

![](graphviz/sender-receiver-onepayoff.pdf){scale=0.5}

## Some information theory (i)

* The degree of uncertainty associated with the World RV can be summarized by
  its *entropy*:
    - Number of yes/no questions one needs to ask in order to know the state of
      the world.
    - If the world can be in many, equiprobable states, entropy is high.
    - If it can be in just a few states, or they are far from equiprobable,
    entropy is lower. 

. . .

$$H(W)= -\sum_i \Pr(W_i)\log_2(\Pr(W_i))$$

## Some information theory (ii)

* A message, $M_i$, is informative about the world insofar as it reduces the entropy of
  the World RV
    - Once you know that the value of the Message RV is $M_i$, you need to ask
    fewer yes/no questions in order to know the value of World.

. . .

$$H(W | M=M_i)= -\sum_j \Pr(W_j | M_i)\log_2(\Pr(W_j | M_i))$$

## What do messages represent? (i)

* Skyrms's *informational content* for a message $M_i$: the vector of *pointwise mutual
  informations* between that message and all states.

    $$\langle\log_2 p(W_1 | M_i) - \log_2 p(W_1), \ldots, \log_2 p(W_n | M_i) - \log_2 p(W_n)\rangle$$


## What do messages represent? (ii)

* *Propositional content* of a message $M$: the disjunction of states such that
  they are not ruled out in the informational content. E.g. "$W_1$ or $W_2$ is
  the case"

. . . 

> The familiar propositional content is a rather special case of the much
> richer information-theoretic account of content [@skyrms_signals_2010, p. 42]

## False propositional contents are impossible

* Messages with false propositional content cannot be produced [@birch_propositional_2014]:

    - A message meaning "$W_1$ or $W_2$ is the case" would be false if produced in
      $S_3$

    - But, if $P(W_3 | M) = 0$ (and $P(W_3) > 0$ and $P(M > 0)$) then $P(M |
      W_3) = 0$

## No explanatory payback

* Simple representations do not earn their explanatory keep
  [@burge_origins_2010; @rescorla_millikan_2013; @schulte_perceptual_2015]

    - But every message in every sender-receiver system has a Skyrmsian
      informational content (and thus a propositional one)

## Improving the model

* I will urge two changes:

1. World structure is relevant: The monolithic *World* random variable should
   be substituted with a collection of RVs

2. Rate distortion theory is the right tool to describe Lewis-Skyrms
   models---if we are interested in representation.

# The rate-distortion approach

## The world is represented in a single random variable (i)

![](graphviz/sender-receiver-onepayoff.pdf){scale=0.5}

* A discrete menu of jointly exhaustive, *mutually exclusive* options.

## The world is represented in a single random variable (ii)

* This is not expressive enough, in a crucial respect:
    - World states have features in common
    - Plausibly, misrepresentation often depends on this (e.g., same detectable
      cues present in the good and the bad cases)
    - (The move from events to random variables in probability theory---two
      independent coins)

## The world is represented in a single random variable (iii)

* We should substitute the monolithic world states in the classical Lewis-Skyrms
  framework with collections of RVs, linked together by a joint probability
  distribution.

    - E.g. 111111 and 000000 are two different states. In the former, 6
      properties are instantiated; in the latter none is.
    - 111111 and 110001 are also different, but partially overlapping, states:
      both are such that properties $P_1$, $P_2$ and $P_6$ are instantiated.

## Rate distortion theory (i)
 
1. The sender is confronted with a set of states composed of property instantiations (represented
   by RVs), governed by a joint probability distribution.

2. They then send one of a discrete, smallish set of messages

3. The receiver decodes the message.

## Rate distortion theory (ii)

* We evaluate the quality of the transmission with a *distortion measure* 
    - The more similar original and decoded message are, the less the distortion.
    - 111111 to 000000: distortion of 6; 111111 to 111110: distortion of 1
    - (By the way, this distortion-measure approach is mathematically equivalent
      to the usual payoff-matrix approach)

## Rate distortion theory (iii)

* The main RDT question: what's the relation between the rate (size of message
  repertoire) and distortion?

## Not so different from the usual approach, really

![](graphviz/sender-receiver-onepayoff.pdf){scale=0.5}
![](graphviz/rdt.pdf){scale=0.6}

## Bottlenecks!

* Usually, signaling repertoires are smallish (perhaps except when repertoire
  size itself is the signal!)
    - In animal communication, often one, sometimes two or three
    - In any event, much smaller than all there is to be said about the world

* The bigger the repertoire (*channel rate*) the lower the distorsion
    - if the channel rate is equal to or larger than the entropy of states,
      then it is possible to communicate losslessly (without distortion)

* (In this paper I am interested in the ideal case---lowest attainable
  distortion, in a situation of perfect common interest.)

## Lossy Compression

* How about this: *Representational content is a form of lossy compression*

* Compression offers an excellent explanatory payback: content attributed to a
  signal would be saying a lot about the signaling arrangement, in an economic
  way

* At least some of our representational practices are sensitive to how good a
  compression can be achieved at various channel rates.


## The rate-distortion function

* Let's start with information generated by tosses of a fair coin.
     - What's the entropy of this system?

* Let's say distortion is 1 if the decoded message is different from the
  original one; 0 otherwise

* What's the minimum distortion at rate 0?
    - i.e., when *no* message is sent.

* What's the minimum rate for which distortion is 0?

## RD function for a fair coin (i)

![](/home/manolo/Models/rate_distortion_signaling/bernoulli_rd.pdf){scale=0.3}

## RD function for a fair coin (ii)

* For a coin toss the more rate you throw at it, the better, till you hit the
  entropy of states. 
    - There are no natural stopping points---no sweet spots

## A more interesting world: two "essence kinds"

![](graphviz/two_indepndent_bitbits.pdf)

* Two "essence kinds" 
    - Think of the parent as a hidden essence, and children as detectable, or
      fitness conducive, properties.

* If parent is instantiated, children have a  .95 probability of being
  instantiated; .05 otherwise

* Parents have a .5 probability of being instantiated

## Sweet spots (i)

![](/home/manolo/Models/rate_distortion_signaling/two_essence_kinds_rd.pdf)

## Sweet spots (ii)

* I suggest that our content-attributing practices are sensitive to this bit of
  informational structure in the problem signaling is solving:

. . . 

> A sender-receiver arrangement is fruitfully described as involving
> representations only if there are sudden drops in the usefulness of extra
> bandwidth ("sweet spots")

## Distortion-minimizing encoding (i)

* And what's the encoding that minimizes distortion at the sweet spot (2 bits)?

    1. Divide the state vector in two halves (one for each kind)
    2. If in the first half there's a majority of 1s, set the first bit to 1
       (else set it to 0)
    3. Ditto for the second half and the second bit
    4. Decode the resulting message substituting 1 by 11111 and 0 by 00000

* E.g., 11100 011000 encodes to 10, which in turn decodes to 11111 00000

## Distortion-minimizing encoding (ii)

![](/home/manolo/Models/rate_distortion_signaling/two_essence_kinds_rd_with_majority.pdf)

## Misrepresentation

* Now, misrepresentation is perfectly possible.

* From a purely informational perspective: whenever the decoded and the
  original message are different.

* From a representational perspective, it's natural to read the
  distortion-minimizing encoding as representing the presence or absence of the
  two essence kinds
    - An instance of a kind might be represented in its absence.
    - When does the kind count as instantiated (presence of the parent node? A majority of its
      properties?) is an open question---but the above is true under any
      reasonable answer.

## Constancy / robustness vs cue-driven behavior (i)

1. Set the first bit to 1 if a certain child node in the first half is 1
2. Ditto for the second half
3. Decode as above.

## Constancy / robustness vs cue-driven behavior (ii)

![](/home/manolo/Models/rate_distortion_signaling/two_essence_kinds_rd_with_child.pdf)

## Constancy / robustness vs cue-driven behavior (ii)

* Constancy / robustness improves the quality of the encoding

* I suggest that our content-attributing practices are sensitive to this
  *other* bit of informational structure:

. . .

> A sender-receiver arrangement is more clearly representational the more it
> approaches the distortion-minimizing encoding

## Broad-banded response (i)

* Now, on the receiver side, is there a similar motivation for the preference
  for broad-banded responses?

* Suppose that the receiver has no use for all that information about the
  state---it just cares about a certain fitness-conducive property.
    - That is, we introduce a new distortion measure: 0 if the value for, e.g.,
      $P_2$ is correctly decoded; 1 otherwise.
    - "1111100000" and "0100011111" have a distortion measure of 0.
    - This is the way magnetotactic bacteria are usually described.

## Broad-banded response (ii)

![](/home/manolo/Models/rate_distortion_signaling/two_essence_simple_rd.pdf)

## Broad-banded response (iii)

* Caring about only one property changes the structure of the problem (to a
  coin toss)

* Using a different detectable property, moreover, lands the encoding way above
  the rate-distortion curve.

* Without sweet spots, and far from the distortion-minimizing encoding, content
  attributions have no explanatory payback.

# Conclusions

## Conclusions

* Information---there's lots of it. But we need more expressive power than
  the usual Dretskean information measures have
    - Shannon's theory is a source-channel theory. We need that.
    - Also, worlds represented as groups of probabilistically connected RVs

* Worlds thus described + agent interests and goals induce a rate-distortion
  problem. I have suggested that some of the explanatory payback of
  representational talk depends on 
  
    1. The existence of natural resting points in the curve, and
    2. The ability of sender and receiver to use an encoding in the vicinity of
       the curve

* Object recognition, alarm calls have this structure --- oxygen-poor water
  detection in magnetotatic bacteria does not.

## A possible mechanism for content attribution

* When researchers are confronted with a simple signaling system they
  (tacitly!) try to find a rate distortion problem it might be solving...

* ... and whether it is actually being solved.

* The clearer it is these two questions are answered in the affirmative, the
  better candidate for representational status this is.

## Three (among very many) outstanding questions

* What happens if we relax the common interest assumption?

* How does this play out in an evolutionary game-theoretic setting?

* Is there a general role for the rate-distortion function to play in
  representation?

## Thanks!
